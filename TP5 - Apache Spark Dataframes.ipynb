{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>TP5 - Apache Spark Dataframes</center>\n",
    "\n",
    "## Intro\n",
    "Pour ce TP vous allez utiliser **spark-notebook** qui a ete précédemment installé dans votre VM.\n",
    "\n",
    "## Preparation de l’environnement\n",
    "> Dans nos exemples l’adresse IP de la VM est ***192.168.56.101. N’oubliez pas de remplacer partout dans les exemples cette adresse par l’adresse affichée dans la console VirtualBox***. Pour acceder a l’interface spark-notebook il faudra rediriger egalement le port 9001 (en plus des ports habituels).\n",
    "```shell\n",
    "[andrei@desktop ~]$ ssh  -L 9080:127.0.0.1:8080 \\\n",
    "                         -L 8081:127.0.0.1:8081 \\\n",
    "                         -L 8082:127.0.0.1:8082 \\\n",
    "                         -L 4040:127.0.0.1:4040 \\\n",
    "                         -L 9001:127.0.0.1:9001 \\\n",
    "                  bigdata@192.168.56.101\n",
    "bigdata@192.168.56.101's password:\n",
    "Last login: Sun Jan  4 14:53:32 2015 from pc12.home\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark notebook\n",
    "* (1) lancer le spark-notebook (depuis le bon repertoire !)\n",
    "```shell\n",
    "[bigdata@bigdata ~]$ cd /home/bigdata/spark-notebook-0.7.0/ \n",
    "[bigdata@bigdata spark-notebook-0.7.0]$ bin/spark-notebook \n",
    "Play server process ID is 4582\n",
    "SLF4J: Class path contains multiple SLF4J bindings.\n",
    "SLF4J: Found binding in [jar:file:/home/bigdata/spark-notebook-0.7.0/lib/ch.qos.logback.logback-classic-1.1.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
    "SLF4J: Found binding in [jar:file:/home/bigdata/spark-notebook-0.7.0/lib/org.slf4j.slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
    "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
    "SLF4J: Actual binding is of type [ch.qos.logback.classic.util.ContextSelectorStaticBinder]\n",
    "[info] play - Application started (Prod)\n",
    "[info] play - Listening for HTTP on /0:0:0:0:0:0:0:0:9001  \n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "* (2) suivez les exercices sur [l’interface du spark-notebook](http://localhost:9001/notebooks/telecom2016/TP6_Dataframes.snb) Vous pouvez editer une cellules en cliquant. Vous pouvez executer le code d’une cellule via le menu/Cell/Run ou via `Shift`+`Enter`. Pour avoir la completion automatique du code vous pouvez utiliser `TAB`. Si le notebook ne reponds pas vous pouvez redemarer le kernel spark via Kernel/Restart ou redemarer le notebook (`Ctrl`+`C` dans le terminal puis relancer bin/spark-notebook).\n",
    "\n",
    "\t\n",
    "> Identifiants de connexion :\n",
    "* utilisateur : ***bigdata***\n",
    "* password : ***bigdatafu***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------\n",
    "## Ressources\n",
    "[Spark Programming Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "\n",
    "[SQL Dataframes Tutorial](http://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "\n",
    "[Scala API](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.package)\n",
    "\n",
    "[Scala Cheat-Sheet](http://homepage.cs.uiowa.edu/~tinelli/classes/022/Fall13/Notes/scala-quick-reference.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------\n",
    "\n",
    "# Notes de cours :\n",
    "* Il vaut mieux utiliser `reduceByKey` que `groupByKey` qui est moins efficace et plus coûteux en calculs\n",
    "\n",
    "* Data serialization : il vaut mieux utiliser `Kryo` pour sérialialiser \n",
    "    ``` scala\n",
    "    val conf = new SparkConf().setMaster(...).setAppName(...)\n",
    "    conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    conf.registerKryoClasses(Array(classOf[MyClass1], classOf[MyClass2]))\n",
    "    val sc = new SparkContext(conf)\n",
    "    ```\n",
    "* Il faut programmer les fonctions en ne mettant rien en dehors d'elle. Toutes les variables doivent être contenues en interne pour éviter la mutation des variables. D'où l'intérêt de définir des variables immutables.\n",
    "\n",
    "* Pour qu'un code fonctionne en production, il faut le tester sur un petit cluster à *minima*, et en prenant un dataset représentatif du volume.\n",
    "\n",
    "* Eviter les `collect()` sur les résultats pour ne pas surcharger le driver. On utilise plutôt `take(nb)`. Ou alors `count()` before `collect` or `sample`.\n",
    "\n",
    "* **Caching** (Mise en cache) : \n",
    "    1. If your RDDs fit in RAM ⇒ **MEMORY_ONLY**\n",
    "    2. If not ⇒ try **MEMORY_ONLY_SER + a fast serialization library**\n",
    "    3. DISK? ⇒ unless the DAG is expensive to compute or filters a large amount of the data !\n",
    "    4. **Replicated storage levels ?** ⇒ fast fault recovery (e.g. using Spark to serve requests from a web application).\n",
    "        1. all the storage levels provide full fault tolerance by recomputing lost data !\n",
    "        2. the replicated ones let you continue running tasks on the RDD without waiting to recompute a lost partition.\n",
    "\n",
    "\n",
    "* **Parallelism level** : \n",
    "    1. `repartion(num_partitions)` and `coalesce(num_partitions)` ⇒ change number of partitions\n",
    "    2. `spark.default.parallelism` ⇒ recommend 2-3 tasks per CPU core in your cluster\n",
    "\n",
    "\n",
    "* **Dataframe performance** :\n",
    "\n",
    "<img src=\"http://andreiarion.github.io/images/dataframe-performance.png\" alt=\"dataframe performance\"/>\n",
    "\n",
    "<img src=\"http://andreiarion.github.io/images/Distributed-Wordcount.png\" alt=\"Distributed Wordcount\"/>\n",
    "\n",
    "## Conclusion\n",
    "* To understand Spark you need to understand RDDs\n",
    "\n",
    "* Use Dataframes / **Datasets** / SparkSQL for the optimisations\n",
    "\n",
    "* Use RDD for custom optimisations\n",
    "\n",
    "* Watch for data shuffles (use the Spark UI)\n",
    "\n",
    "* Use the documentation: http://spark.apache.org/docs/latest/programming-guide.html\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
