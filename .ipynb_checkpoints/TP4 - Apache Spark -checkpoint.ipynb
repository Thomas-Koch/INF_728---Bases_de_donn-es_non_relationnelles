{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "# <center>- TP4 : Apache Spark -</center>\n",
    "------------------------------\n",
    "## Intro\n",
    "Pour le TP Spark vous allez utiliser une machine virtuelle qui contient un cluster Cassandra de 3 noeuds ainsi que Apache Spark. Le cluster Spark est composé d’un master et d’un noeud worker avec deux executeurs ([documentation deployement standalone](https://spark.apache.org/docs/2.0.2/spark-standalone.html)) Vous allez vous connecter a cette machine via ***ssh***.\n",
    "\n",
    ">Pour ce TP vous avez besoin d’une machine (idéalement linux) avec 4GB de RAM et 4GB d’espace disque disponible. ***VirtualBox*** et un client ***SSH*** doivent être déjà installés. Si vous êtes sous Windows vous pouvez utiliser http://www.putty.org/\n",
    "\n",
    "### Connexion en ssh sur la VM depuis votre machine physique\n",
    ">Pour pouvoir accéder à l’interface SparkUI nous allons rediriger 4 ports :\n",
    "* le port de SparkUI pour le Spark shell (4040) ⇒ sera dirigé sur votre machine physique localhost:4040\n",
    "* le port qui contient l’UI du master spark (8080) ⇒ sera dirigé sur votre machine physique localhost:9080\n",
    "* le port qui contient l’UI du premier worker (8081) ⇒ sera dirigé sur votre machine physique localhost:8081\n",
    "* le port qui contient l’UI du deuxiem worker (8082) ⇒ sera dirigé sur votre machine physique localhost:8082\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 :\n",
    "Connectez vous en ssh sur la VM depuis votre machine physique (un terminal linux/ putty sous windows). On profite pour rediriger les ports locaux de votre machine physique vers les ports de la VM :\n",
    "```shell\n",
    "[andrei@desktop ~]$ ssh  -L 9080:127.0.0.1:8080 \\\n",
    "                         -L 8081:127.0.0.1:8081 \\\n",
    "                         -L 8082:127.0.0.1:8082 \\\n",
    "                         -L 4040:127.0.0.1:4040 \\\n",
    "                  bigdata@192.168.56.101\n",
    "bigdata@192.168.56.101's password:\n",
    "Last login: Sun Jan  4 14:53:32 2015 from pc12.home\n",
    "[bigdata@bigdata ~]$\n",
    "```\n",
    ">Identifiants de connexion :\n",
    "* utilisateur : ***bigdata***\n",
    "* password : ***bigdatafuret***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "## Démarrage du cluster Spark\n",
    "Chaque application Spark utilise un programme *driver* qui sert a lancer des calculs distribués sur un cluster. Ce programme définit les structures de données distribuées dans le cluster ainsi que le DAG d’opérations sur ces structures de données. Un noeud coordinateur(appele aussi master) lancera ces operations sur deux noeuds Worker qui hebergeront un/plusieurs executeur(s).\n",
    "\n",
    "<img src=\"http://andreiarion.github.io/images/cluster-overview.png\" alt=\"cluster overview\"/>\n",
    "\n",
    "\n",
    "### Configuration de l'environnement\n",
    "Rajouter dans le script de configuration de l’environement spark (conf/spark-env.sh) la ligne ***export SPARK_WORKER_INSTANCES=2*** puis démarrer le master et les workers via le script ***spark-2.0.2-bin-hadoop2.7/sbin/start-all.sh***\n",
    "\n",
    "```shell\n",
    "[bigdata@bigdata ~]$[bigdata@bigdata ~]$ cat spark-2.0.2-bin-hadoop2.7/conf/spark-env.sh | grep SPARK_WORKER_INSTANCES #1\n",
    "# - SPARK_WORKER_INSTANCES, to set the number of worker processes per node\n",
    "export SPARK_WORKER_INSTANCES=2\n",
    "\n",
    "\n",
    "[bigdata@bigdata ~]$ spark-2.0.2-bin-hadoop2.7/sbin/start-all.sh #2 \n",
    "starting org.apache.spark.deploy.master.Master, logging to /home/bigdata/spark-2.0.2-bin-hadoop2.7/logs/spark-bigdata-org.apache.spark.deploy.master.Master-1-bigdata.out\n",
    "localhost: starting org.apache.spark.deploy.worker.Worker, logging to /home/bigdata/spark-2.0.2-bin-hadoop2.7/logs/spark-bigdata-org.apache.spark.deploy.worker.Worker-1-bigdata.out\n",
    "localhost: starting org.apache.spark.deploy.worker.Worker, logging to /home/bigdata/spark-2.0.2-bin-hadoop2.7/logs/spark-bigdata-org.apache.spark.deploy.worker.Worker-2-bigdata.out\n",
    "```\n",
    "\n",
    "1. vérifier la configuration de l’environnement\n",
    "2. on utilise le script start-all.sh pour démarrer le master et les workers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Shell\n",
    "* Une fois avoir demarré le master Spark et les Workers, on se connecte via le shell Spark:\n",
    "```shell\n",
    "[bigdata@bigdata ~]$ spark-2.0.2-bin-hadoop2.7/bin/spark-shell\\\n",
    "                            --master spark://bigdata:7077\n",
    "```\n",
    "\n",
    "<img src=\"http://andreiarion.github.io/images/spark-shell.png\" alt=\"spark shell\"/>\n",
    "\n",
    "\n",
    "* Notez le port utilisé par SparkUI dans les messages de démarrage:\n",
    "\n",
    "Dans ce qui suit on va utiliser le shell Spark comme driver.               \n",
    "\n",
    "Pour se connecter au cluster le ***shell Spark*** nous a crée automatiquement un objet **sc** de type *SparkContext* :\n",
    "\n",
    "```scala\n",
    "scala> sc //1\n",
    "res0: org.apache.spark.SparkContext = org.apache.spark.SparkContext@3f10439f\n",
    "\n",
    "scala> sc.getConf.toDebugString //2\n",
    "res1: String =\n",
    "hive.metastore.warehouse.dir=file:/home/bigdata/spark-warehouse\n",
    "spark.app.id=app-20161204081840-0000\n",
    "spark.app.name=Spark shell\n",
    "spark.driver.host=192.168.56.101\n",
    "spark.driver.port=33541\n",
    "spark.executor.id=driver\n",
    "spark.home=/home/bigdata/spark-2.0.2-bin-hadoop2.7\n",
    "spark.jars=\n",
    "spark.master=spark://bigdata:7077\n",
    "spark.repl.class.outputDir=/tmp/spark-d120dc7d-9f40-4b5d-99ec-91cb46425e2d/repl-e18a9b14-f2d4-4f23-b916-bd61e3dd983b\n",
    "spark.repl.class.uri=spark://192.168.56.101:33541/classes\n",
    "spark.sql.catalogImplementation=hive\n",
    "spark.submit.deployMode=client\n",
    "```\n",
    "1. afficher le type de l’objet\n",
    "2. ***sc.getConf.toDebugString*** permet d’afficher la configuration du cluster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkUI\n",
    "L’interface SparkUI permet de voir la configuration et l’état du cluster Spark.\n",
    "\n",
    "Une fois la mise en place des 4 tunnels ssh (4040,8080,8081,8082) vous pouvez vous connecter à :\n",
    "\n",
    "* l’interface SparkUI qui tourne sur le master : http://localhost:9080 (permettra de connaitre l’etat du master, voir les jobs et logs)\n",
    "\n",
    "<img src=\"http://andreiarion.github.io/images/sparkUI-on-master.png\" alt=\"sparkUI on master\"/>\n",
    "\n",
    "\n",
    "* l’interface du Worker 1 : http://localhost:8081 (permettra de connaitre l’etat du worker, voir les jobs et logs)\n",
    "\n",
    "<img src=\"http://andreiarion.github.io/images/sparkUI-on-worker1.png\" alt=\"sparkUI on worker1\"/>\n",
    "\n",
    "\n",
    "* l’interface du Worker 2 : http://localhost:8082 (permettra de connaitre l’etat du worker, voir les jobs et logs)\n",
    "\n",
    "<img src=\"http://andreiarion.github.io/images/sparkUI-on-worker2.png\" alt=\"sparkUI on worker2\"/>\n",
    "\n",
    "\n",
    "* l’interface du driver http://localhost:4040 (permettra de suivre l’execution de nos programmes Spark)\n",
    "\n",
    "<img src=\"http://andreiarion.github.io/images/sparkUI-on-driver.png\" alt=\"sparkUI on driver\"/>\n",
    "\n",
    "\n",
    "Vérifier dans SparkUI que tous les noeuds worker sont connectés. Quelles sont les informations disponibles sur le cluster? Combien de noeuds sont dans votre cluster?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------\n",
    "## *Spark Shell*\n",
    "### Exercices Scala\n",
    "Le *Spark shell* est un shell Scala dans lequel des classes supplémentaires ont été pré-importées et qui construit automatiquement un contexte Spark pour gérer l’échange avec un cluster. Dans *Spark shell* on peut donc écrire n’importe quelle instruction Scala.\n",
    "\n",
    "```scala\n",
    "scala> val myNumbers = List(1, 2, 5, 4, 7, 3) //1\n",
    "myNumbers: List[Int] = List(1, 2, 5, 4, 7, 3)\n",
    "\n",
    "scala> def cube(a: Int): Int = a * a * a //2\n",
    "cube: (a: Int)Int\n",
    "\n",
    "scala> myNumbers.map(x => cube(x)) //3\n",
    "res2: List[Int] = List(1, 8, 125, 64, 343, 27)\n",
    "\n",
    "scala> myNumbers.map{x => x * x * x} //4\n",
    "res3: List[Int] = List(1, 8, 125, 64, 343, 27)\n",
    "\n",
    "scala> def even(a:Int):Boolean = { a%2 == 0 } //5 \n",
    "even: (a: Int)Boolean\n",
    "\n",
    "scala> myNumbers.map(x=>even(x)) //6\n",
    "res4: List[Boolean] = List(false, true, false, true, false, false)\n",
    "\n",
    "scala> myNumbers.map(even(_)) //7\n",
    "res5: List[Boolean] = List(false, true, false, true, false, false)\n",
    "\n",
    "scala> myNumbers.filter(even(_)) //8\n",
    "res6: List[Int] = List(2, 4)\n",
    "\n",
    "scala> myNumbers.foldLeft //9\n",
    "\n",
    "def foldLeft[B](z: B)(f: (B, A) => B): B\n",
    "\n",
    "scala> myNumbers.foldLeft(0)(_+_) //10 \n",
    "res10: Int = 22\n",
    "\n",
    "scala> myNumbers.foldLeft(10)(_+_) //10 \n",
    "res10: Int = 32\n",
    "```\n",
    "1. définir une variable immutable(*val*) de type List[Int]\n",
    "2. définir une fonction qui prend en entrée un entier et retourne le cube\n",
    "3. utiliser la fonction map pour appliquer un traitement (ici la fonction cube est appliquée à chaque element de la liste)\n",
    "4. utilisation d’une fonction anonyme pour faire le même traitement\n",
    "5. définition d’une fonction Int→Boolean qui retourne si un nombre est pair\n",
    "6. on transforme la liste des entiers dans une liste de booleans\n",
    "7. la même chose qu’avant mais en utilisant une écriture plus compacte\n",
    "8. filtrage des numéros paires de la liste\n",
    "9. dans le shell on peut utiliser l’aide via la touche `TAB`. Ecrire myNumbers.foldLeft puis appuyer sur `TAB` permet d’afficher la signature de la function [foldLeft](http://www.arolla.fr/blog/2011/10/listes-scala-methodes-foldleft-et-foldright/) de scala. Elle prend deux listes d’arguments ⇒ un élèment neutre de type B et une fonction f: (B, A) ⇒ B. L'élément neutre est en fait le point de départ du calcul. Ici il prendra par exemple 0 pour l'ajouter au premier élément de la liste, retournera le résultat et s'en servira comme nouvel élément neutre pour le second élément de la liste. Et ainsi de suite.\n",
    "10. utilisation de foldLeft pour calculer la somme des élèments de la liste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ecrire une courte séquence Scala qui affiche les nombres inférieurs à 1000 à la fois divisibles par 2 et par 13.*\n",
    "\n",
    "**A.** Version non parallelisée (Scala uniquement)\n",
    "```scala\n",
    "scala> val data = 1 to 1000 //1\n",
    "data: scala.collection.immutable.Range.Inclusive = Range(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170...\n",
    "scala> val filteredData= data.filter(n=> (n%2==0) && (n%13==0) ) //2\n",
    "filteredData: scala.collection.immutable.IndexedSeq[Int] = Vector(26, 52, 78, 104, 130, 156, 182, 208, 234, 260, 286, 312, 338, 364, 390, 416, 442, 468, 494, 520, 546, 572, 598, 624, 650, 676, 702, 728, 754, 780, 806, 832, 858, 884, 910, 936, 962, 988)\n",
    "```                                                         \n",
    "1. génerer une séquence (du type Scala `scala.collection.immutable.Range.Inclusive`) de tous les nombres de 1 à 1000. Cette séquence sera stockée dans la variable immutable *data*\n",
    "2. garder les nombres qui sont divisible par 2 et 13\n",
    ">Dans l’exemple précedent on n’a pas utilisé le context Spark (l’objet sc). Les objets qu’on a manipulé sont les objets des [collections standard Scala](http://www.scala-lang.org/api/current/#scala.collection.package)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercices RDDs\n",
    "***1.1*** *Utiliser Spark pour distribuer les données et le filtrage de l’exercice precedent sur le cluster Spark.*\n",
    "```scala\n",
    "scala> val data = 1 to 1000 //1\n",
    "data: scala.collection.immutable.Range.Inclusive = Range(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170...\n",
    "scala> val paralelizedData= sc.parallelize(data) //2\n",
    "paralelizedData: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:14\n",
    "\n",
    "scala> val filteredData= paralelizedData.filter(n=> (n%2==0) && (n%13==0)) //3\n",
    "filteredData: org.apache.spark.rdd.RDD[Int] = FilteredRDD[1] at filter at <console>:16\n",
    "\n",
    "scala> filteredData.collect //4\n",
    "...\n",
    "Array[Int] = Array(26, 52, 78, 104, 130, 156, 182, 208, 234, 260, 286, 312, 338, 364, 390, 416, 442, 468, 494, 520, 546, 572, 598, 624, 650, 676, 702, 728, 754, 780, 806, 832, 858, 884, 910, 936, 962, 988)                                                         \n",
    "```                                                         \n",
    "1. géneration des données (! dans la JVM du driver/shell Spark)\n",
    "2. créer un ***RDD*** (*paralelizedData*) qui va contenir les données (distribuées sur les noeuds du cluster)\n",
    "3. créer un ***RDD*** (*filteredData*) qui va contenir les données filtrées (distribuées sur les noeuds du cluster)\n",
    "4. lancer l’action (*collect*) qui va déclencher le traitement parallèle et va retourner le résultat. Suivre dans le UI l’execution de votre script Spark http://localhost:4040.\n",
    "\n",
    "***1.2*** *Ecrire une courte séquence Scala/Spark qui affiche la somme des nombres paires de 1 a 1000 (utilisez la fonction `fold` disponible sur un RDD qui est similaire a [foldLeft](http://www.arolla.fr/blog/2011/10/listes-scala-methodes-foldleft-et-foldright/) de scala).*\n",
    "```scala\n",
    "scala> val sumPair= paralizedData.filter(n=> (n%2==0)).fold(0)(_+_)\n",
    "sumPair: Int = 250500   \n",
    "```\n",
    "\n",
    "***1.3*** *Ecrire une courte séquence Scala/Spark qui affiche le produit des nombres paires de 1 a 1000.*\n",
    "```scala\n",
    "scala> val prodPair= paralelizedData.filter(n=> (n%2==0)).fold(1)(_*_)\n",
    "sumPair: Int = 0 //on sort des capacités de calcul  \n",
    "```\n",
    "\n",
    "***1.4*** *Combiner les deux derniers programmes en un seul. Mettre en cache les données au niveau des noeuds et vérifier la répartition des ces données via le Spark UI*\n",
    "\n",
    "```scala\n",
    "scala> val parDataCached = sc.parallelize(1 to 1000).filter(_%2==0).cache  //1\n",
    "parDataCached: org.apache.spark.rdd.RDD[Int] = FilteredRDD[6] at filter at <console>:13\n",
    "\n",
    "scala> parDataCached.fold(0)(_+_)  //2\n",
    "res7: Int = 250500\n",
    "\n",
    "scala> parDataCached.collect.foldLeft(BigInt(1))(_*_) //2\n",
    "res8: Int = 399909909090900778289489289... //le nombre retourné est égal\n",
    "```\n",
    "1. définition d’un RDD qui va stocker tous les nombres pairs sur les noeuds\n",
    "2. ré-utilisation du RDD pour calculer la somme et le produit de ces nombres\n",
    "\n",
    "\n",
    "Vous pouvez vérifier sur les noeuds le contenu du RDD mis en cache via le SparkUI (http://localhost:4040/storage/ puis cliquer sur le lien dans la colonne RDD Name)\n",
    "\n",
    "<img src=\"http://andreiarion.github.io/images/cached-rdd-1.png\" alt=\"cached rdd 1\"/>\n",
    "\n",
    "*Liste des RDD mise en cache*\n",
    "\n",
    "<img src=\"http://andreiarion.github.io/images/cached-rdd-2.png\" alt=\"cached rdd 2\"/>\n",
    "\n",
    "*Détails sur la mide en cache*\n",
    "\n",
    "***1.5*** *WordCount : écrire le programme pour compter l’occurence des mots du fichier /home/bigdata/spark-2.0.2-bin-hadoop2.7/README.md.*\n",
    "```scala\n",
    "val textFile = sc.textFile(\"/home/bigdata/spark-2.0.2-bin-hadoop2.7/README.md.\")\n",
    "val counts = textFile.flatMap(line => line.split(\" \"))\n",
    "                 .map(word => (word, 1))\n",
    "                 .reduceByKey(_+_)\n",
    "counts.saveAsTextFile(\"/home/bigdata/spark-2.0.2-bin-hadoop2.7/\") \n",
    "counts.collect.foreach(println)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "***1.6*** *Ecrire le programme qui donne le mot le plus souvent utilisé du fichier (vous pouvez utiliser sortByKey après avoir inversé la liste des paires (mot,nb_occurences)). Suivez via le SparkUI les stage d’executions de votre traitement (http://localhost:4040/)*\n",
    "```scala\n",
    "val sort = counts.sortBy(_._2, false) //on trie selon la deuxième variable\n",
    "sort.collect\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkSQL\n",
    "***3.1*** *Utiliser SparkSQL pour trouver dans le fichier /home/bigdata/spark-2.0.2-bin-hadoop2.7/examples/src/main/resources/people.txt les noms des personnes qui ont moins de 19 ans*\n",
    "\n",
    "```scala\n",
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc) //1\n",
    "\n",
    "\n",
    "case class Person(name: String, age: Int) //2\n",
    "\n",
    "\n",
    "val people = sc.textFile(\"/home/bigdata/spark-2.0.2-bin-hadoop2.7/examples/src/main/resources/people.txt\").map(_.split(\",\")).map(p => Person(p(0), p(1).trim.toInt)).toDF //3\n",
    "\n",
    "people.createOrReplaceTempView(\"people\") //4\n",
    "\n",
    "\n",
    "val teenagers = sql(\"SELECT * FROM people WHERE age<20\") //5\n",
    "\n",
    "teenagers.show //6\n",
    "```\n",
    "1. création d’un contexte *SQLContext*\n",
    "2. définition du modèle de données en utilisant une *case classe* Scala\n",
    "3. création d’un dataframe Spark a partir d’un RDD\n",
    "4. enregistrement du dataframe dans une table temporaire\n",
    "5. requêtage sur la table → A COMPLETER !\n",
    "6. le résultat d’une requête *SparkSQL* est un DataFrame, nous utilisons la methode show to afficher son contenu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark et Cassandra\n",
    ">Démarrer votre cluster Cassandra et vérifier qu’il est démarré:\n",
    "\n",
    "```shell\n",
    "[bigdata@bigdata ~]$  ccm status; ccm start; ccm status\n",
    "Cluster: 'cassandra-2.1.16'\n",
    "---------------\n",
    "node1: DOWN\n",
    "node3: DOWN\n",
    "node2: DOWN\n",
    "Cluster: 'cassandra-2.1.16'\n",
    "---------------------------\n",
    "node1: UP\n",
    "node3: UP\n",
    "node2: UP\n",
    "```\n",
    "\n",
    "Pour se connecter a Cassandra depuis le shell Spark il faut arrêter le shell (`Ctrl`+`C` or `Ctrl`+`D`) puis le redémarrer en ajoutant en paramètre l’adresse d’un noeud Cassandra et la dependence vers le connecteur spark-cassandra :\n",
    "```shell\n",
    "[bigdata@bigdata ~]$ spark-2.0.2-bin-hadoop2.7/bin/spark-shell --master spark://bigdata:7077  --conf spark.cassandra.connection.host=127.0.0.1 --packages datastax:spark-cassandra-connector:2.0.0-M2-s_2.11\n",
    "```\n",
    "\n",
    "**3.1** WordCount avec *Spark* et *Cassandra*\n",
    "```scala\n",
    "scala> import com.datastax.spark.connector._ //1\n",
    "import com.datastax.spark.connector._\n",
    "\n",
    "scala> val rdd = sc.cassandraTable(\"temperature\", \"temp1\") //2 \n",
    "rdd: com.datastax.spark.connector.rdd.CassandraTableScanRDD[com.datastax.spark.connector.CassandraRow] = CassandraTableScanRDD[0] at RDD at CassandraRDD.scala:18\n",
    "\n",
    "scala> rdd.collect //3\n",
    "res0: Array[com.datastax.spark.connector.CassandraRow] = Array(CassandraRow{ville: Paris, date: 2016-12-01 00:00:00+0100, temperature: 4}, CassandraRow{ville: Rennes, date: 2016-12-01 00:02:00+0100, temperature: 8}, CassandraRow{ville: Rennes, date: 2016-12-01 00:01:00+0100, temperature: 7}, CassandraRow{ville: Paris, date: 2016-12-01 00:02:00+0100, temperature: 6}, CassandraRow{ville: Rennes, date: 2016-12-01 00:00:00+0100, temperature: 6}, CassandraRow{ville: Paris, date: 2016-12-01 00:01:00+0100, temperature: 5})\n",
    "\n",
    "scala>import com.datastax.spark.connector.cql.CassandraConnector //4\n",
    "\n",
    "scala>CassandraConnector(sc.getConf).withSessionDo { session =>\n",
    "    session.execute(s\"CREATE KEYSPACE IF NOT EXISTS demo WITH REPLICATION = {'class': 'SimpleStrategy', 'replication_factor': 1 }\")\n",
    "    session.execute(s\"CREATE TABLE IF NOT EXISTS demo.wordcount (word TEXT PRIMARY KEY, count COUNTER)\")\n",
    "    session.execute(s\"TRUNCATE demo.wordcount\")\n",
    "  } //5\n",
    "\n",
    "scala>val rdd= sc.textFile(\"/home/bigdata/spark-2.0.2-bin-hadoop2.7/README.md\").flatMap(_.split(\"[^a-zA-Z0-9']+\")).map(word => (word.toLowerCase, 1)).reduceByKey(_ + _).filter(x => x._1!=\"\") //6\n",
    "\n",
    "scala>rdd.collect.foreach(println(_))\n",
    "\n",
    "scala> rdd.saveToCassandra(\"demo\",\"wordcount\") < //7\n",
    "\n",
    "scala> sc.cassandraTable(\"demo\", \"wordcount\").collect //8\n",
    "```\n",
    "\n",
    "1. importer toutes les classes du pacakge com.datastax.spark.connector.\n",
    "2. créer un RDD a partir de la table de temperatures.\n",
    "3. collecter le RDD et afficher les temperatures\n",
    "4. import de la classe CassandraConnector du package cql\n",
    "5. creer un nouveau keyspace et une nouvelle table\n",
    "6. créer un RDD avec les occurence des mots du fichier README.md\n",
    "7. sauver ce RDD dans la table Cassandra *wordcount*\n",
    "8. afficher le contenu de la table *wordcount*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ressources\n",
    "\n",
    "[Spark Programming Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "\n",
    "[SQL Dataframes Tutorial](http://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "\n",
    "[Scala API](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.package)\n",
    "\n",
    "[Python API](http://spark.apache.org/docs/latest/api/python/index.html)\n",
    "\n",
    "[Spark Cassandra Connector](https://github.com/datastax/spark-cassandra-connector)\n",
    "\n",
    "[Scala Cheat-Sheet](http://homepage.cs.uiowa.edu/~tinelli/classes/022/Fall13/Notes/scala-quick-reference.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
